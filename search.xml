<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[tensorflow2.0初探]]></title>
    <url>%2F2019%2F03%2F21%2Ftensorflow2.0%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[2019.3.7日，Google发布了Tensorflow2.0alpha0版本，尝试使用了此版本，是为记录 安装cpu版本安装方法： 1pip install tensorflow==2.0.0-alpha0 gpu版本安装方法： 1pip install tensorflow-gpu==2.0.0-alpha0 注意tensorflow2.0的gpu版本是用cuda10.0编译的，所以安装tensorflow2.0的机器上必须装有cuda10.0。如果在装有cuda9.x及以下版本的机器上装tensorflow2.0，安装时不会报错，但在import tensorflow时会报错，提示找不到对应的cublas库。 核心改变相对于tensorflow1.x，tensorflow2.0引入的改变很多，这些改变的目的大多是使tensorflow2.0更易于使用[3]： Eager Execution 将是 2.0 的核心特性。它将用户对编程模型的期望与 TensorFlow 实践更好地结合起来，使 TensorFlow 更易于学习和应用。 支持更多的平台和语言，通过交换格式的标准化和 API 的对齐，改进这些组件之间的兼容性和对等性。 删除弃用的 API 并减少重复，避免给用户带来混乱。 公开的 2.0 设计过程：社区现在可以与 TensorFlow 开发人员合作，使用 TensorFlow 讨论组讨论新特性。 兼容性和延续性：提供一个与 TensorFlow 1.x 兼容的模块，这意味着 TensorFlow 2.0 将有一个包含所有 TensorFlow 1.x API 的模块。 On-disk 兼容性：TensorFlow 1.x 中导出的模型（检查点和冻结模型）将与 TensorFlow 2.0 兼容，只需要重命名某些变量。 tf.contrib：完全删除。大型的维护中的模块将移动到独立的存储库；未使用和未维护的模块将被删除。 对从2.0开始学习tensorflow的学习者来说，这些改变确实使得tensorflow的易用性提高了。但对于tensorflow1.x的使用者来说，要重新适应各种新的api，构建模型的思维方式也要改变，之前的代码要想升级到2.0，也要做大幅的修改，这并不是让人愉快的改变。对于这些人来说，要么转向其他平台，如pytorch，要么接受并学习这些改变。然而对于Android系的移动设备算法开发者来说，tensorflow还是不二的选择，于是接受tensorflow2.0的改变就是唯一可行的路了。 下面以最经典的两个机器学习任务为例，介绍下tensorflow2.0的使用，2.0中静态图的方式仍然是被支持的，但Eager Execution模式将是默认的模式，所以我们的例子以Eager Execution模式为例。 入门示例-线性回归[2]线性回归的简单解释：给定一个数据集{(x1, y1), (x2, y2), … }，拟合出一个线性模型，使得这个模型能尽可能的反映出x和y之间的关系。其中x可以是一维的标量，也可以是多维的向量。 最简单的情况就是x为一维标量的情况，这时，数据集中的每一项都是二维平面中的一个点。拟合出的线性模型可以表示为二维平面中的一条直线。我们就以tensorflow2.0写一个此种情形的线性回归模型。 tensorflow中训练一个模型的步骤主要有如下几步： 数据准备 定义模型和损失函数 训练 线性回归模型也不例外 数据准备假设要回归的线性模型是y = 5x + 2，那么我们可以用以下代码生成在这条线附近的1000个点 12345678910import tensorflow as tfW_TRUE = 5B_TRUE = 2# 生成数据DATA_NUM = 1000inputs = tf.random.normal(shape=[DATA_NUM])noise = tf.random.normal(shape=[DATA_NUM])outputs = W_TRUE * inputs + B_TRUE + noise 定义模型和损失函数12345678910W = tf.Variable(-1.0) # 需要训练的参数1，初始值设为-1b = tf.Variable(-1.0) # 需要训练的参数2，初始值设为-1# 模型def model(inputs): return W * inputs + b # 平面中的直线方程# 损失函数，使用均方误差损失函数def loss(y_true, y_pred): return tf.reduce_mean(tf.square(y_true - y_pred)) 训练因为数据很少，所以我们在每个epoch里一次性计算所有的数据点，不做batch。 因为模型简单，也没有使用优化器，而是直接采用最基本的梯度下降算法。 使用以下代码训练50个epoch，并在每个epoch末尾打印W, b的值和损失值 123456789EPOCH_NUM = 50learning_rate = 0.1for epoch in range(EPOCH_NUM): with tf.GradientTape() as tape: #tf.GradientTape()是一个自动梯度计算器 loss_value = loss(outputs, model(inputs)) #计算损失函数的返回值 dW, db = tape.gradient(loss_value, [W, b]) #计算W和b的梯度 W.assign_sub(learning_rate * dW) #参数W的值减去学习率乘以W的梯度 b.assign_sub(learning_rate * db) #参数b的值减去学习率乘以b的梯度 print("--epoch %d, W = %.2f, b = %.2f, loss = %.2f" %(epoch, W.numpy(), b.numpy(), loss_value.numpy())) 50个epoch后，最后一次打印： 1--epoch 49, W = 5.02, b = 2.03, loss = 0.96 可以看到W为5.02，b为2.03，和真实的W_TRUE，B_TRUE，非常接近了。之所以没有得到和真实的W_TRUE、B_TRUE完全相同的结果，是因为我们的数据本身是加了噪音的，这些噪音并非是在y = 5x + 2两侧完全均衡的。 入门示例-逻辑回归[2]逻辑回归的简单解释：和线性回归类似，给定一个数据集{(x1, y1), (x2, y2), … }，和线性回归不同的是，这里的y的值只有有限的几种类别，拟合出一个线性模型，使得这个模型的结果能正确预测出对于一个未知的x，相应的y的分类。其中x可以是一维的标量，也可以是多维的向量。 数据准备这个例子中，我们的数据采用经典minist数据，minist数据在tensorflow2.0内有专门的API可以访问。 12345678910111213141516171819202122import tensorflow as tffrom tensorflow.keras import datasets, optimizers, layersimport numpy as npbatch_size = 600 #minist数据集比较大，一般需要分batch进行计算，我们设置batch大小为600# Import MNIST data(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() #使用tensorflow.keras.datasets接口来加载mnist数据print(x_train.shape)train_dataset = ( tf.data.Dataset.from_tensor_slices((tf.reshape(x_train, [-1, 784]), y_train)) #x_train中的数据是60000×28×28的，要resize成60000×784 .batch(batch_size) #分成大小为600的batch .shuffle(1000) #打乱数据，让数据更均匀)train_dataset = ( train_dataset.map(lambda x, y: (tf.divide(tf.cast(x, tf.float32), 255.0), #归一化，将x_train的数据归一化到0到1之间 tf.reshape(tf.one_hot(y, 10), (-1, 10)))) #y_train是0到9的单数字标签，将其转换为0到9的one hot标签) 定义模型和损失函数模型使用和线性回归类似的模型，只是这里的W和b不再是标量，而是多维向量，而且因为是分类模型，所以在计算完x*W+b后要对其做softmax分类 12345678# 定义模型的参数W = tf.Variable(tf.zeros([784, 10])) #要训练的参数b = tf.Variable(tf.zeros([10])) #要训练的参数# 构建模型model = lambda x: tf.nn.softmax(tf.matmul(x, W) + b) # Softmax# 交叉熵损失函数compute_loss = lambda true, pred: tf.reduce_mean(tf.reduce_sum(tf.losses.binary_crossentropy(true, pred), axis=-1)) 训练这里的梯度下降算法优化器，使用了Adam，它能使梯度收敛得更快。 这里的学习率也比之前的线性回归模型的学习率要低两个数量级，因为相对来说这是一个复杂的模型，而且Adam本身就能加速收敛，它的初始学习率不宜太高。 123456789101112131415161718# 计算准确度compute_accuracy = lambda true, pred: tf.reduce_mean(tf.keras.metrics.categorical_accuracy(true, pred))learning_rate = 0.001 # 学习率training_epochs = 6 # 训练的epoch数目# 梯度下降算法的优化器，使用Adam优化器optimizer = tf.optimizers.Adam(learning_rate)for epoch in range(training_epochs): for i, (x_, y_) in enumerate(train_dataset): with tf.GradientTape() as tape: #tf.GradientTape()是一个自动梯度计算器 pred = model(x_) #计算输出 loss = compute_loss(y_, pred) #计算损失 acc = compute_accuracy(y_, pred) #计算准确率，只是为了看结果，对训练本身无影响 grads = tape.gradient(loss, [W, b]) #计算梯度 optimizer.apply_gradients(zip(grads, [W, b])) #使用梯度下降优化器来更新W和b print("=&gt; loss %.2f acc %.2f" %(loss.numpy(), acc.numpy())) 6个epoch后，得到的结果： 1=&gt; loss 28.26 acc 0.93 准确率0.93，已经是个不错的分类器。 入门示例-神经网络分类[1]上面的minist分类的例子，如果用神经网络来做，会更简单，效果也更好，下面我们就用最基本的全连接神经网络来实现下。照例是数据准备、模型和损失函数定义、训练三个步骤。其中数据准备和上面逻辑回归的例子几乎是完全一样的，只是不用把y变为one_hot，不再赘述。 模型和损失函数定义构建一个三层的全连接网络，其中前两层各有128个神经元，第三层有10个神经元。 损失函数采用交叉熵损失函数。 123456789101112model = tf.keras.Sequential([ layers.Dense(128, activation='relu'), layers.Dense(128, activation='relu'), layers.Dense(10) ]);model.build(input_shape=(None, 784))# model.summary()def compute_loss(logits, labels): return tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits( logits=logits, labels=labels)) 训练这里的梯度下降算法优化器，使用了Adam，参数默认，没有指定学习率。 其他的步骤和逻辑回归基本是一样的 12345678910111213141516def compute_accuracy(logits, labels): predictions = tf.argmax(logits, axis=1) return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))optimizer = optimizers.Adam()for epoch in range(6): for step, (x,y) in enumerate(train_dataset): with tf.GradientTape() as tape: logits = model(x) loss = compute_loss(logits, y) acc = compute_accuracy(logits, y) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) print('epoch:', epoch, 'loss:', loss.numpy(), 'acc:', acc.numpy()) 同样训练6个epoch, 准确率能到0.96 1epoch: 5 loss: 0.13207151 acc: 0.9683333 如果训练20个epoch，准确率能到1.0。 总结从上面的几个例子可以看出，我们全程都使用了默认的Eager Execution模式，而且这个模式不需要像tensorflow1.x一样显式的声明。所以我们的代码中不再有Session.run，而是随定义随执行，代码比较简洁，理解也非常容易。 参考资料https://github.com/dragen1860/TensorFlow2.0Tutorials https://github.com/YunYang1994/TensorFlow2.0-Examples https://www.tinymind.cn/articles/3844]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode 刷题记录-56.Merge Intervals]]></title>
    <url>%2F2019%2F02%2F22%2Fleetcode%20%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95-56.Merge%20Intervals%2F</url>
    <content type="text"><![CDATA[题目Given a collection of intervals, merge all overlapping intervals. Example 1: 123Input: [[1,3],[2,6],[8,10],[15,18]]Output: [[1,6],[8,10],[15,18]]Explanation: Since intervals [1,3] and [2,6] overlaps, merge them into [1,6]. Example 2: 123Input: [[1,4],[4,5]]Output: [[1,5]]Explanation: Intervals [1,4] and [4,5] are considered overlapping. 说明给定一组表示区间的列表，把其中所有有重叠的区间合并，输出新的一组区间列表 分析解题思路首先对输入的列表按照start值由小到大排序。然后取已排序列表的第一个区间的end和下一个区间的start对比，如果有重叠，则更新区间的end为这两个区间的end中较大的一个；如果没有重叠，则把更新后的区间增加到返回列表中。然后继续与已排序列表中的下一个对比，直到列表的末尾。 复杂度分析排序的时间复杂度为O(nlogn), 列表逐元素对比的时间复杂度为O(n)，所以总体的时间复杂度为O(nlogn)。 代码123456789101112131415161718192021222324252627282930/** * Definition for an interval. * public class Interval &#123; * int start; * int end; * Interval() &#123; start = 0; end = 0; &#125; * Interval(int s, int e) &#123; start = s; end = e; &#125; * &#125; */class Solution &#123; public List&lt;Interval&gt; merge(List&lt;Interval&gt; intervals) &#123; if(intervals.size() &lt;= 1) return intervals; intervals.sort((i1, i2) -&gt; Integer.compare(i1.start, i2.start)); List&lt;Interval&gt; result = new LinkedList&lt;Interval&gt;(); int start = intervals.get(0).start; int end = intervals.get(0).end; for(Interval interval : intervals)&#123; if(interval.start &lt;= end)&#123; end = Math.max(interval.end, end); &#125;else&#123; result.add(new Interval(start, end)); start = interval.start; end = interval.end; &#125; &#125; result.add(new Interval(start, end)); return result; &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列和管道]]></title>
    <url>%2F2019%2F02%2F21%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%92%8C%E7%AE%A1%E9%81%93%2F</url>
    <content type="text"><![CDATA[linux系统中多进程通信的方法有管道、消息队列、信号量、共享内存、套接字等。 本文记录下最近用到的消息队列和管道 消息队列定义消息队列消息队列是从一个进程发送数据块到另一个进程的一种方法。数据块存放在一个消息结构体中，消息结构体需要自定义。 消息队列的使用使用消息队列主要使用如下三个函数： msgget1int msgget(key_t, key, int msgflg); 用于根据key获取一个消息队列的id，msgflg表示消息队列的访问权限，和文件的访问权限一致。如果消息队列不存在，且msgflg和IPC_CREATE做或操作，那么会创建一个新的消息队列并返回这个新消息队列的id。所以msgget也用于创建消息队列。 msgsnd1int msgsnd(int msgid, const void *msg_ptr, size_t msg_sz, int msgflg); 用于发送一条消息到id为msgid的消息队列中去，各参数的说明如下： 其中msgid是由msgget创建的消息队列id。 msg_ptr指针指向消息结构体，这个消息结构体需要自己定义且需要遵循一定的规则： 结构体的第一个元素必须是个长整型变量，用于指示消息类型，消息类型后面会详细解释 结构体的其他元素可以自定义，用于传输自己需要的数据。传输数据的大小是有限制的，在Linunx中，一条消息的最大长度为MSGMAX 如： 12345struct msg_st&#123; long int msg_type; char msg_text[32];&#125;; msg_sz是消息结构体中的消息数据的长度 msgflg用于控制当消息队列满了或超过系统限制时发生的事情，通常设为0 msgrcv1int msgrcv(int msgid, void *msg_ptr, size_t msg_st, long int msgtype, int msgflg); msgrcv用于接收消息队列，它的参数大多和msgsnd一致，只是多了一个msgtype参数，这个参数和消息结构体中的消息类型配合使用 msgctl1int msgctl(int msgid, int command, struct msgid_ds *buf); 用于获取和设置消息队列的属性 command命令有： IPC_STAT ： 获取消息队列的数据到buf中 IPC_SET ：为消息队列设置属性，属性事先存在buf中。可设置的属性包括：msg_perm.uid、msg_perm.gid、msg_perm.mode以及msg_qbytes IPC_RMID：删除消息队列 消息类型消息结构中的消息类型表示要发送的消息的类型，消息类型的作用与接收消息的进程的msgrcv的参数有关： 如果msgrcv参数中的msgtype设置为0，则会接收对应消息队列中的第一条消息，无论这条消息的消息类型是什么。 如果msgrcv参数中的msgtype设置为大于0，则只能接收对应消息队列中消息类型和msgrcv参数中的msgtype一致的消息。 消息队列示例123456789101112131415161718192021222324252627282930313233343536373839404142//发送消息队列int send_message(int msg_key)&#123; struct msg_st msgdata; int msgid = -1; msgdata.msg_type = 1; sprintf(msgdata.msg_text, "%d", frame_second); msgid = msgget((key_t)msg_key, 0666 | IPC_CREAT); if(msgid == -1) &#123; return -1; &#125; if(msgsnd(msgid, (void*)&amp;msgdata, sizeof(msgdata.msg_text), 0) == -1) &#123; return -1; &#125; return 0;&#125;//接收消息队列并删除之int receive_msg( int msg_key)&#123; int msgid = msgget((key_t)msg_key, 0666 | IPC_CREAT); long int msgtype = 0; struct msg_st msgdata; if(msgid == -1) &#123; return -1; &#125; if(msgrcv(msgid, (void*)&amp;msgdata, sizeof(msgdata.msg_text), msgtype, 0) == -1) &#123; return -1; &#125; if(msgctl(msgid, IPC_RMID, 0) == -1) &#123; return -1; &#125; return 0;&#125; 管道定义管道管道，顾名思义，是数据从一端输入另一端输出的通信机制，管道里的数据是流式的，不可反复读取。 它仅适用于有公共祖先的两个进程，具有公共祖先的两个进程可以是父子进程，也可以是由同一父进程fork出的兄弟进程。 普通的管道是半双工的，即只能一端输入，另一端输出，数据是单向流动的。另外有“流管道”是支持全双工的，本文仅介绍半双工的普通管道。 生成管道管道的生成方式很简单，只需要调用pipe函数，pipe函数的原型： 12#include &lt;unistd.h&gt;int pipe(int pipefd[2]); //成功返回0，失败返回-1并设置errno pipe函数的参数是长度为2的int数组，调用pipe函数成功后，数组内存储着管道的两个文件描述符，其中pidfd[0]为读文件描述符，pipefd[1]为写文件描述符。这两个文件描述符都是已经打开的，不需要对其调用open函数。但需要在使用的时候用close函数关闭掉。 pipe的创建是在主进程内进行的，假设要创建一个父子进程间的管道，当主进程使用fork函数创建出子进程时，在主进程和子进程里各有一对pipeid，都可以进行读写。这时要根据实际需要决定是在主进程内读子进程内写还是与之相反。无论哪种情况，都要在两个进程内把用不到的文件描述符close，保留需要的文件描述符。 管道的几种状态写管道时： 如果管道读端已经关闭 进程异常终止，或者如果程序中捕获SIGPIPE信号，这样进程就不会终止了 如果管道读端没有关闭 如果管道已经写满，阻塞，等待管道有空 如果管道没有写满，则正常写入 读管道时： 如果管道写端已经关闭 read返回0，类似读文件读到文件末尾 如果管道写端没有关闭 如果管道内有数据，read返回读到的字节数 如果管道内没有数据，阻塞直到管道内有数据 管道示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;int main(void)&#123; int pipefd[2]; int res = pipe(pipefd); int pid; if(res == -1)&#123; printf("create pipe failed!"); &#125; pid = fork(); switch (pid) &#123; case -1: /* failure */ if(pipefd[0] != -1)&#123; close(pipefd[0]); &#125; if(pipefd[1] != -1)&#123; close(pipefd[1]); &#125; printf("fork failed"); break; case 0: /* child */ if (pipefd[0] != -1) &#123; close(pipefd[0]); &#125; dup2(pipefd[0], STDOUT_FILENO); //将ls的结果写入管道中 execlp("ls", "ls", "-lrt", NULL); //ls输出结果默认对应屏幕 default: /* parent */ if (pipefd[1] != -1) &#123; close(pipefd[1]); &#125; dup2(pipefd[1], STDIN_FILENO); //让wc从管道中读取数据 execlp("wc", "wc", "-l", NULL); //wc命令默认从标准读入取数据 &#125; return 0;&#125; 消息队列和管道的对比 消息队列可以用于不具有亲缘关系的进程，而管道只能用于具有亲缘关系的进程 二者都是用发送和接收的方式传输数据]]></content>
      <categories>
        <category>编程基础</category>
      </categories>
      <tags>
        <tag>多进程通信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileFaceNets论文阅读笔记]]></title>
    <url>%2F2018%2F05%2F02%2FMobileFaceNets%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文链接：MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices 核心改进点用 MobileNetV1, ShuffleNet, MobileNetV2等网络做人脸识别时，在embedding层之前往往采用global average pooling得到特征，但其他论文已经提出使用global average pooling后的CNN效果会比不使用global average pooling差，本文从人脸识别的角度对这个结论做了解释：假如在embedding层之前，特征的输出是7×7×512，这时7×7的范围内，角点和中心点对人脸特征的贡献是不同的，中心点对人脸特征的贡献更大。但如果使用global average pooling，则相当于把7×7个点的每个点的贡献视为均等的，这就降低了模型的特征提取能力。 如果不采用global average pooling，也可以采用全连接层，但这样会大大增加计算量，并不适合用于移动端设备。 本文对这个问题的改进方法是在embedding层之前增加depth-wise conv层，假如特征的输出是7×7×512，那么增加的depth-wise conv层的核就是7×7×512，增加的层和特征输出卷积计算就会得到1×1×512的层，可以把这个层直接做为embedding层，但在本文的标准MobileFaceNets模型中，其后增加了一个1×1的卷积层。 本文用实验结果证明，在MobileNetV2网络上，仅改变这一点，就能得到比MobileNetV2效果好的模型。后面本文在MobileNetV2的基础上设计了效果更好的MobileFaceNets系列网络。 网络模型本文的网络模型是基于MobileNetV2改进的，基本和结构和MobileNetV2几乎一样： 实验效果ArcFace loss MS-Celeb-1M database [5] with 3.8M images from 85K subjects： MobileFaceNet， LFW acc 99.55%，AgeDB Acc. 96.07%，Megaface acc 90.39, val(1e-6) 92.59 在所有的适用于移动端的网络中，效果是最好的，模型也比MobileNetV2 MobileNetV1要小。 但不考虑移动端计算能力因素，目前效果最好的人脸识别网络是iBUG_DeepInsight (ArcFace [5] LResNet100EIR)：LFW acc 99.83%， AgeDB Acc 98.08%， Megaface acc 98.06, val(1e-6) 98.48 关于MobileNetV2mobilenetV2在延续mobilenetv1的depth-wise卷积的基础上，使用了resnet的shortcut连接，但并不是直接在depth-wise卷积块的前后加上shortcut连接，mobilenetV2的bottleneck块的主要思想如下： 因为普通residual block一般是先用1×1卷积压缩通道数，然后再做卷积，再用1×1卷积扩展通道数。如果在depth-wise卷积之前像这样先压缩通道，那么效果会很差，因为depth-wise卷积依赖于较多的通道数。所以mobilenetV2使用了和普通residual block相反的策略，先扩展通道数，再做depth-wise卷积，然后压缩通道数，并称之为Inverted residuals。 bottleneck块最后，并不使用Relu来做非线性操作，而是直接输出Linear bottlenecks。这是因为在做了扩展-depth-wise卷积-压缩操作后，特征已经被压缩，这时再做Relu会破坏特征，实验证明，不做Relu能提升效果]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
</search>
